{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMalK_oYR-X_"
   },
   "source": [
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 1: –í–∏–∫–ª–∏–∫ LLM –∑ –±–∞–∑–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
    "\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –≤–∏–∫–ª–∏–∫–∞—Ç–∏ LLM –∑—ñ –∑–≤–∏—á–∞–π–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–≤–∏–º –ø—Ä–æ–º–ø—Ç–æ–º.\n",
    "\n",
    "–ü—Ä–æ–º–ø—Ç –º–∞—î –¥–æ–∑–≤–ª—è—Ç–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ—Å—Ç–æ—é –º–æ–≤–æ—é –Ω–∞ –ø–µ–≤–Ω—É —Ç–µ–º—É. –í —Ü—å–æ–º—É –∑–∞–≤–¥–∞–Ω–Ω—ñ –º–∏ —Ö–æ—á–µ–º–æ –¥—ñ–∑–Ω–∞—Ç–∏—Å—å –ø—Ä–æ —Ç–µ–º—É \"–ö–≤–∞–Ω—Ç–æ–≤—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è\".\n",
    "\n",
    "–í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ –ø–æ–≤–∏–Ω–Ω–∞ –º—ñ—Å—Ç–∏—Ç–∏ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è, –∫–ª—é—á–æ–≤—ñ –ø–µ—Ä–µ–≤–∞–≥–∏ —Ç–∞ –ø–æ—Ç–æ—á–Ω—ñ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –≤ —Ü—ñ–π –≥–∞–ª—É–∑—ñ.\n",
    "\n",
    "–û–±–º–µ–∂—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –¥–æ 200 —Å–∏–º–≤–æ–ª—ñ–≤ —ñ –ø—Ä–æ–ø–∏—à—ñ—Ç—å –≤ –ø—Ä–æ–º–ø—Ç—ñ –∞–±–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±—É–ª–∞ –∫–æ—Ä–æ—Ç–∫–æ—é (—Ü–µ –∑–µ–∫–æ–Ω–æ–º–∏—Ç—å –í–∞–º —á–∞—Å —ñ –≥—Ä–æ—à—ñ –Ω–∞ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ —Ç–æ–∫–µ–Ω–∏).\n",
    "\n",
    "–í —è–∫–æ—Å—Ç—ñ LLM –º–æ–∂–Ω–∞ —Å–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏—Å—å —è–∫ –º–æ–¥–µ–ª–ª—é –∑ HugginFace (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é Mistral), —Ç–∞–∫ —ñ ChatGPT4 –∞–±–æ ChatGPT3. –í –æ–±–æ—Ö –≤–∏–ø–∞–¥–∫–∞—Ö —Ç—Ä–µ–±–∞ —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –ø–æ—Ç—Ä—ñ–±–Ω—É \"–æ–±–≥–æ—Ä—Ç–∫—É\" (—Ç–æ–±—Ç–æ –∫–ª–∞—Å, —è–∫–∏–π –¥–æ–∑–≤–æ–ª–∏—Ç—å —ñ–Ω—ñ—Ü—ñ—é–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å) –∑ LangChain –¥–ª—è –≤–∏–∫–ª–∏–∫—É LLM –∑–∞ API, –∞ —Ç–∞–∫–æ–∂ –∑—á–∏—Ç–∞—Ç–∏ –æ—Å–æ–±–∏—Å—Ç–∏–π —Ç–æ–∫–µ–Ω –∑ —Ñ–∞–π–ª–∞, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, `creds.json`, —è–∫–∏–π —Ä–æ–∑–º—ñ—â–µ–Ω–∏–π —É –í–∞—Å –ª–æ–∫–∞–ª—å–Ω–æ —ñ –í–∏ –ù–ï –∑–¥–∞—î—Ç–µ –π–æ–≥–æ –≤ –î–ó —ñ –ù–ï –∫–æ–º—ñ—Ç–∏—Ç–µ –≤ git üòè\n",
    "\n",
    "–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —Å–≤–æ—î –∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏ –Ω–∞ —Å–≤—ñ–π —Ä–æ–∑—Å—É–¥ (—Ç—É—Ç –Ω–µ–º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —á–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è) —ñ –Ω–∞–ø–∏—à—ñ—Ç—å, —á–æ–º—É –≤–∏ –æ–±—Ä–∞–ª–∏ —Å–∞–º–µ —Ç–∞–∫–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è —Ü—å–æ–≥–æ –∑–∞–≤–¥–∞–Ω–Ω—è.  \n",
    "\n",
    "–ó–∞–ø–∏—Ç–∏ –º–æ–∂–Ω–∞ —Ä–æ–±–∏—Ç–∏ —è–∫ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é, —Ç–∞–∫ —ñ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—é - –æ—Ä—ñ—î–Ω—Ç—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ, –¥–µ —ñ —á–∏ —Ö–æ—á–µ—Ç–µ –≤–∏ –ø–æ—Ç—ñ–º –ª–∏—à–∏—Ç–∏ —Ü–µ–π –ø—Ä–æ—î–∫—Ç —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∞ —è–∫–∞ –º–æ–≤–∞ –±—É–¥–µ –ø–∞—Å—É–≤–∞—Ç–∏ –±—ñ–ª—å—à–µ. –í —Ä–æ–∑–≤ º—è–∑–∫–∞—Ö –ø—Ä–æ–º–ø—Ç–∏ - —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import Tool, AgentExecutor, AgentType, create_react_agent, initialize_agent\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from huggingface_hub import InferenceClient\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAIa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as file:\n",
    "  creds = json.load(file)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"HF_TOKEN\"] = creds['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–≤–∞–Ω—Ç–æ–≤–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è - —Ü–µ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø'—é—Ç–µ—Ä–Ω–∏—Ö –Ω–∞—É–∫, —è–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –ø—Ä–∏–Ω—Ü–∏–ø–∏ –∫–≤–∞–Ω—Ç–æ–≤–æ—ó –º–µ—Ö–∞–Ω—ñ–∫–∏ –¥–ª—è —Ä–æ–∑—Ä–æ–±–∫–∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤ —Ç–∞ –ø—Ä–∏—Å—Ç—Ä–æ—ó–≤, —è–∫—ñ –º–æ–∂—É—Ç—å –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à–µ –æ–±—Ä–æ–±–ª—è—Ç–∏ –≤–µ–ª–∏–∫—ñ –æ–±'—î–º–∏ –¥–∞–Ω–∏—Ö, –Ω—ñ–∂ –∫–ª–∞—Å–∏—á–Ω—ñ –∫–æ–º–ø'—é—Ç–µ—Ä–∏. –¶—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏ —Ç–∞ –ø—Ä–∏—Å—Ç—Ä–æ—ó –º–æ–∂—É—Ç—å –∑–Ω–∞—á–Ω–æ —Å–∫–æ—Ä–æ—Ç–∏—Ç–∏ —á–∞—Å –æ–±—Ä–æ–±–∫–∏ —Ç–∞ –∑–º–µ–Ω—à–∏—Ç–∏ –µ–Ω–µ—Ä–≥–æ–≤–∏—Ç—Ä–∞—Ç–∏, –æ—Å–æ–±–ª–∏–≤–æ –¥–ª—è –∑–∞–¥–∞—á, —è–∫—ñ –º–∞—é—Ç—å –≤–µ–ª–∏–∫—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π.\n"
     ]
    }
   ],
   "source": [
    "overal_temperature = 0.1\n",
    "messages = [{\"role\": \"user\", \"content\": \"–ù–∞–¥–∞–π –∫–æ—Ä–æ—Ç–∫–µ –ª–∞–∫–æ–Ω—ñ—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–≤–∞–Ω—Ç–æ–≤–∏–º –æ–±—á–∏—Å–ª–µ–Ω–Ω—è–º.\"}]\n",
    "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "print(client.chat_completion(messages,\n",
    "                             temperature=overal_temperature,\n",
    "                             max_tokens=200\n",
    "                            ).choices[0].message.content\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiIzV0UIS0GA"
   },
   "source": [
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É\n",
    "–¢–µ–ø–µ—Ä –º–∏ —Ö–æ—á–µ–º–æ –æ–Ω–æ–≤–∏—Ç–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π —Ñ—É–∫–Ω—Ü—ñ–æ–Ω–∞–ª —Ç–∞–∫, –∞–±–∏ –≤ –ø—Ä–æ–º–ø—Ç –º–∏ –º–æ–≥–ª–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç–∏ —Ç–µ–º—É —è–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä. –î–ª—è —Ü—å–æ–≥–æ —Å–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ—Å—å `PromptTemplate` –∑ `langchain` —ñ —Ä–µ–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–∏–π –ø—Ä–æ–º–ø—Ç —Ç–∞ –≤–∏–∫–ª–∏–∫ –º–æ–¥–µ–ª—ñ –∑ –Ω–∏–º.\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç—ñ—Ç—å –æ–Ω–æ–≤–ª–µ–Ω–∏–π —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª (–ø—Ä–æ–º–ø—Ç + –º–æ–¥–µ–ª) –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å –ø—Ä–æ —Ç–µ–º–∏\n",
    "- \"–ë–∞—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
    "- \"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\"\n",
    "- \"Explainable AI\"\n",
    "\n",
    "–í–∏–≤–µ–¥—ñ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—ñ–¥–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–∞ –µ–∫—Ä–∞–Ω."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"—Ç–µ–º–∞\"],\n",
    "    template=\"–ù–∞–¥–∞–π –∫–æ—Ä–æ—Ç–∫–µ –ª–∞–∫–æ–Ω—ñ—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–µ–º–∏ {—Ç–µ–º–∞}.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë–∞–π—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ - —Ü–µ –≥—Ä—É–ø–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤, —è–∫—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å —Ç–µ–æ—Ä—ñ—é –ë–∞—î—Å–∞ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –∑–∞–¥–∞—á –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó, —Ä–µ–≥—Ä–µ—Å—ñ—ó —Ç–∞ —ñ–Ω—à–∏—Ö –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è. –í–æ–Ω–∏ –æ—Ü—ñ–Ω—é—é—Ç—å –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ –¥–∞–Ω–∏–π –ø—Ä–∏–∫–ª–∞–¥ –Ω–∞–ª–µ–∂–∏—Ç—å –¥–æ –ø–µ–≤–Ω–æ–≥–æ –∫–ª–∞—Å—É, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –ø–æ–ø–µ—Ä–µ–¥–Ω—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ä–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤ —Ç–∞ –ø–æ–º–∏–ª–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó. –¶—ñ –º–µ—Ç–æ–¥–∏ –≤—ñ–¥–æ–º—ñ —Å–≤–æ—î—é –∑–¥–∞—Ç–Ω—ñ—Å—Ç—é –æ–±—Ä–æ–±–ª—è—Ç–∏ –Ω–µ–ø–æ–≤–Ω—É —Ç–∞ –Ω–µ—Ç–æ—á–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é, –∞ —Ç–∞–∫–æ–∂ –¥–æ–∑–≤–æ–ª—è—é—Ç—å –≤—Ä–∞—Ö–æ–≤—É–≤–∞—Ç–∏ –∫–æ—Ä–∏—Å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–∏—Ö —Ç–∞ —ó—Ö–Ω—ñ –≤–∑–∞—î–º–æ–∑\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": prompt.format(—Ç–µ–º–∞=\"–ë–∞–π—î—Å—ñ–≤—Å—å–∫—ñ –º–µ—Ç–æ–¥–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\")}]\n",
    "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "print(client.chat_completion(messages,\n",
    "                             temperature=overal_temperature,\n",
    "                             max_tokens=200\n",
    "                            ).choices[0].message.content\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ–º–∞ Transformers –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ —Å—Ç–æ—Å—É—î—Ç—å—Å—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂, —è–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –º–µ—Ö–∞–Ω—ñ–∑–º —Å–∞–º–æ—É–≤–∞–≥–∏ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏—Ö –¥–∞–Ω–∏—Ö, —Ç–∞–∫–∏—Ö —è–∫ —Ç–µ–∫—Å—Ç, –∑–≤—É–∫ –∞–±–æ —ñ–º—ñ–¥–∂—ñ. –¶—è –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –¥–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—ñ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –º—ñ–∂ —Ä—ñ–∑–Ω–∏–º–∏ –µ–ª–µ–º–µ–Ω—Ç–∞–º–∏ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ —Ç–∞ –≤–∏–∫–æ–Ω—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—ñ, —Ç–∞–∫—ñ —è–∫ –ø–µ—Ä–µ–∫–ª–∞–¥ –º–æ–≤, –≤–∏—Ä—ñ–∑–∫—É –∑ —Ç–µ–∫—Å—Ç—É, —Å–∏–Ω—Ç–µ–∑ –∑–≤—É–∫—É —Ç–∞ —ñ–Ω.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": prompt.format(—Ç–µ–º–∞=\"–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ –≤ –º–∞—à–∏–Ω–Ω–æ–º—É –Ω–∞–≤—á–∞–Ω–Ω—ñ\")}]\n",
    "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "print(client.chat_completion(messages,\n",
    "                             temperature=overal_temperature,\n",
    "                             max_tokens=200\n",
    "                            ).choices[0].message.content\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainable AI (XAI) –æ–∑–Ω–∞—á–∞—î —à—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç, —è–∫–∏–π –º–æ–∂–µ –ø–æ—è—Å–Ω—é–≤–∞—Ç–∏ —Å–≤–æ—ó —Ä—ñ—à–µ–Ω–Ω—è —Ç–∞ –ø—Ä–æ—Ü–µ—Å–∏, —â–æ –ª–µ–∂–∞—Ç—å –ø–æ–∑–∞–¥—É –Ω–∏—Ö, –¥–ª—è –ª—é–¥–∏–Ω–∏. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —è–∫ AI –ø—Ä–∏–π–º–∞—î —Ä—ñ—à–µ–Ω–Ω—è —Ç–∞ –Ω–∞–¥–∞—î –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏ —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –π–æ–≥–æ —Ä–æ–±–æ—Ç—É.\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": prompt.format(—Ç–µ–º–∞=\"Explainable AI\")}]\n",
    "client = InferenceClient(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "print(client.chat_completion(messages,\n",
    "                             temperature=overal_temperature,\n",
    "                             max_tokens=200\n",
    "                            ).choices[0].message.content\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9UsL2gXSe-3"
   },
   "source": [
    "\n",
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 3: –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –ø—Ä–æ—Ü–µ—Å—ñ–≤\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–æ–∂–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ—Å—Ç–∞–Ω–Ω—ñ –Ω–∞—É–∫–æ–≤—ñ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –≤ —Ä—ñ–∑–Ω–∏—Ö –≥–∞–ª—É–∑—è—Ö. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –∞–≥–µ–Ω—Ç –º–∞—î –∑–Ω–∞–π—Ç–∏ 5 –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π –Ω–∞ —Ç–µ–º—É —à—Ç—É—á–Ω–æ–≥–æ —ñ–Ω—Ç–µ–ª–µ–∫—Ç—É.\n",
    "\n",
    "**–ö—Ä–æ–∫–∏:**\n",
    "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞ —Ç–∏–ø—É ReAct –≤ LangChain –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤.\n",
    "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –ø—Ä–æ–º–ø—Ç, —è–∫–∏–π —Å–ø—Ä—è–º–æ–≤—É—î –∞–≥–µ–Ω—Ç–∞ —à—É–∫–∞—Ç–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—ñ –∞–±–æ –≤ –±–∞–∑–∞—Ö –¥–∞–Ω–∏—Ö –Ω–∞—É–∫–æ–≤–∏—Ö –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π.\n",
    "3. –ê–≥–µ–Ω—Ç –ø–æ–≤–∏–Ω–µ–Ω –≤–∏–¥–∞—Ç–∏ —Å–ø–∏—Å–æ–∫ –ø—É–±–ª—ñ–∫–∞—Ü—ñ–π, –∫–æ–∂–Ω–∞ –∑ —è–∫–∏—Ö –º—ñ—Å—Ç–∏—Ç—å –Ω–∞–∑–≤—É, –∞–≤—Ç–æ—Ä—ñ–≤ —ñ –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å.\n",
    "\n",
    "–î–ª—è –≤–∑–∞—î–º–æ–¥—ñ—ó –∑ –ø–æ—à—É–∫–æ–º —Ç–∞–º –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ `Tool`. –í –ª–µ–∫—Ü—ñ—ó –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏ `serpapi`. –ú–æ–∂–Ω–∞ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—Ç–∏—Å—å –Ω–∏–º, –∞–±–æ –æ–±—Ä–∞—Ç–∏ —ñ–Ω—à–µ –ê–†–Ü –¥–ª—è –ø–æ—à—É–∫—É (–≤–æ–Ω–∏ –≤ —Ç–æ–º—É —á–∏—Å–ª—ñ —î –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω—ñ). –ü–µ—Ä–µ–ª—ñ–∫ —Ä—ñ–∑–Ω–∏—Ö –ê–†–Ü, –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –≤ langchain, —ñ –æ—Ä—ñ—î–Ω—Ç–∏—Ä –ø–æ –≤–∞—Ä—Ç–æ—Å—Ç—ñ –∑–∞–ø–∏—Ç—ñ–≤ –º–æ–∂–Ω–∞ –∑–Ω–∞–π—Ç–∏ –≤ –æ–∫—Ä–µ–º–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—ñ [—Ç—É—Ç](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
    "\n",
    "–õ–∏—à–∞—é —Ç–∞–∫–æ–∂ –Ω–∏–∂—á–µ –ø—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–¥–Ω–æ–≥–æ –∑ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏—Ö –ø–æ—à—É–∫–æ–≤–∏—Ö –ê–†–Ü - DuckDuckGo (–Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–æ–∫–µ–Ω–∞!)  - –º–æ–∂–ª–∏–≤–æ –≤—ñ–Ω –≤–∞–º —Å–ø–æ–¥–æ–±–∞—î—Ç—å—Å—è :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jzj4PHP9XwH-",
    "outputId": "08118bad-e4c6-4873-feb0-e3890dc5c6a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", temperature=overal_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "tools = [Tool(\n",
    "    name=\"search\",\n",
    "    description=\"Use it for searching information on the Internet.\",\n",
    "    func=search.run,\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\py3921\\lib\\site-packages\\langsmith\\client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2288: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to search for the latest publications on Artificial Intelligence.\n",
      "Action: search\n",
      "Action Input: 'Find the 5 latest publications on Artificial Intelligence. Provide the title, authors, short description.'\n",
      "\u001b[36;1m\u001b[1;3mFind the latest research papers and news in Artificial Intelligence. Read stories and opinions from top researchers in our research community. Your privacy, your choice. We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. ... Platforms such as ArXiv.org have gained popularity in the AI community because they provide immediate access to the latest research, allowing the AI community to engage with new ideas at an early stage. Access to preprint has increased visibility and citations of open-access papers, enabling open community feedback and collaboration. Articles are linked to countries based on the authors' listed organizations. If at least one author is from an institution in a country, the article counts for that country. If authors are from multiple countries, the article counts once for each country, but only once per country even if there are multiple authors from the same place. 2. Artificial Intelligence Journal. The Artificial Intelligence Journal has played a pivotal role in the evolution of artificial intelligence research since its inception. Established in the early days of AI, this journal has consistently published new studies shaping people's understanding of AI concepts and methodologies. 1. Becoming Human. Website https://becominghuman.ai/ + Follow Find the latest news, info and tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity at Becoming Human Magazine.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2288: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I have the information I need.\n",
      "Final Answer:\n",
      "1. Title: Becoming Human\n",
      "   Authors: Not specified\n",
      "   Short Description: Website providing the latest news, info, and tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data, and their impact on humanity.\n",
      "\n",
      "2. Title: Artificial Intelligence Journal\n",
      "   Authors: Not specified\n",
      "   Short Description: A journal that has played a pivotal role in the evolution of artificial intelligence research since its inception, publishing new studies shaping people's understanding of AI concepts and methodologies.\n",
      "\n",
      "3. Title: Find the latest research papers and news in Artificial Intelligence\n",
      "   Authors: Not specified\n",
      "   Short Description: A search for the latest research papers and news in Artificial Intelligence, providing immediate access to the latest research and enabling open community feedback and collaboration.\n",
      "\n",
      "4. Title: Access to preprint has increased visibility and citations of open-access papers\n",
      "   Authors: Not specified\n",
      "   Short Description: A discussion on the benefits of open-access papers, including increased visibility and citations, and the role of platforms like ArXiv.org in the AI community.\n",
      "\n",
      "5. Title: Articles are linked to countries based on the authors' listed organizations\n",
      "   Authors: Not specified\n",
      "   Short Description: An explanation of how articles are linked to countries based on the authors' listed organizations, with each article counting once per country even if there are multiple authors from the same place.</s>\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Find the 5 latest publications on Artificial Intelligence.Provide the title, authors, short description.',\n",
       " 'output': \"1. Title: Becoming Human\\n   Authors: Not specified\\n   Short Description: Website providing the latest news, info, and tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data, and their impact on humanity.\\n\\n2. Title: Artificial Intelligence Journal\\n   Authors: Not specified\\n   Short Description: A journal that has played a pivotal role in the evolution of artificial intelligence research since its inception, publishing new studies shaping people's understanding of AI concepts and methodologies.\\n\\n3. Title: Find the latest research papers and news in Artificial Intelligence\\n   Authors: Not specified\\n   Short Description: A search for the latest research papers and news in Artificial Intelligence, providing immediate access to the latest research and enabling open community feedback and collaboration.\\n\\n4. Title: Access to preprint has increased visibility and citations of open-access papers\\n   Authors: Not specified\\n   Short Description: A discussion on the benefits of open-access papers, including increased visibility and citations, and the role of platforms like ArXiv.org in the AI community.\\n\\n5. Title: Articles are linked to countries based on the authors' listed organizations\\n   Authors: Not specified\\n   Short Description: An explanation of how articles are linked to countries based on the authors' listed organizations, with each article counting once per country even if there are multiple authors from the same place.</s>\"}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({'input': \"Find the 5 latest publications on Artificial Intelligence.\"\n",
    "                                \"Provide the title, authors, short description.\"\n",
    "                                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOqujC6qY_NY"
   },
   "source": [
    "\n",
    "\n",
    "### –ó–∞–≤–¥–∞–Ω–Ω—è 4: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–≥–µ–Ω—Ç–∞-–ø–æ–º—ñ—á–Ω–∏–∫–∞ –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –±—ñ–∑–Ω–µ—Å-–∑–∞–¥–∞—á\n",
    "\n",
    "–°—Ç–≤–æ—Ä—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –¥–æ–ø–æ–º–∞–≥–∞—î –≤–∏—Ä—ñ—à—É–≤–∞—Ç–∏ –∑–∞–¥–∞—á—ñ –±—ñ–∑–Ω–µ—Å-–∞–Ω–∞–ª—ñ—Ç–∏–∫–∏. –ê–≥–µ–Ω—Ç –º–∞—î –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É —Å—Ç–≤–æ—Ä–∏—Ç–∏ –ø—Ä–æ–≥–Ω–æ–∑ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ä—ñ–∫ –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —Ä—ñ–≤–µ–Ω—å —ñ–Ω—Ñ–ª—è—Ü—ñ—ó —ñ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏. –ê–≥–µ–Ω—Ç –º–∞—î –≤–º—ñ—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Python —ñ —Ö–æ–¥–∏—Ç–∏ –≤ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç –∞–±–∏ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ.\n",
    "\n",
    "**–ö—Ä–æ–∫–∏:**\n",
    "1. –ù–∞–ª–∞—à—Ç—É–π—Ç–µ –∞–≥–µ–Ω—Ç–∞, —è–∫–∏–π –ø—Ä–∞—Ü—é–≤–∞—Ç–∏–º–µ –∑ –∞–Ω–∞–ª—ñ—Ç–∏—á–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏, –∑–∞–¥–∞–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–º. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –ø–∏—à–µ\n",
    "\n",
    "```\n",
    "–ú–∏ –µ–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –∑ –ë—Ä–∞–∑–∏–ª—ñ—ó. –í 2021 –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞–ª–∏ 200—Ç, –≤ 2022 - 190—Ç, –≤ 2023 - 210—Ç, –≤ 2024 —è–∫–∏–π —â–µ –Ω–µ –∑–∞–∫—ñ–Ω—á–∏–≤—Å—è - 220—Ç. –ó—Ä–æ–±–∏ –æ—Ü—ñ–Ω–∫—É —Å–∫—ñ–ª—å–∫–∏ –º–∏ –∑–º–æ–∂–µ–º–æ –µ–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ –∞–ø–µ–ª—å—Å–∏–Ω—ñ–≤ –≤ 2025 –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –≤ –ë—Ä–∞–∑–∏–ª—ñ—ó —ñ –ø–æ–ø–∏—Ç –Ω–∞ –∞–ø–µ–ª—å—Å–∏–Ω–∏ –≤ —Å–≤—ñ—Ç—ñ –≤–∏—Ö–æ–¥—è—á–∏ –∑ –µ–∫–æ–Ω–æ–º—ñ—á–Ω–æ—ó —Å–∏—Ç—É–∞—Ü—ñ—ó.\n",
    "```\n",
    "\n",
    "2. –°—Ç–≤–æ—Ä—ñ—Ç—å –∑–∞–ø–∏—Ç –¥–æ –∞–≥–µ–Ω—Ç–∞, —â–æ –º—ñ—Å—Ç–∏—Ç—å —á—ñ—Ç–∫–µ –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äì –≤–∏–¥–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—ñ–∑–Ω–µ—Å –∞–Ω–∞–ª—ñ–∑—É –∞–±–æ –Ω–∞–ø–∏—Å–∞—Ç–∏, —â–æ –≤—ñ–Ω –Ω–µ –º–æ–∂–µ —Ü—å–æ–≥–æ –∑—Ä–æ–±–∏—Ç–∏ —ñ –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–ø—Ä–æ—Å—Ç–æ –º–æ–∂–µ –±—É—Ç–∏ –≤—Å–µ –æ–¥–Ω–∏–º –ø–æ–≤—ñ–¥–æ–º–ª–ª–µ–Ω–Ω—è–º).\n",
    "\n",
    "3. –ó–∞–ø—É—Å—Ç—ñ—Ç—å –∞–≥–µ–Ω—Ç–∞ —ñ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏. –©–æ –º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "UK6KA9PyRS0L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", temperature=overal_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "search_tool = Tool(\n",
    "    name=\"search\",\n",
    "    description=\"Use it for searching the economic and weather forecast on the Internet.\",\n",
    "    func=search.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_repl = PythonREPL()\n",
    "python_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands.\"\n",
    "                \"Input should be a valid python command.\"\n",
    "                \"If you want to see the output of a value, you necessarily should print it out with `print(...)`.\"\n",
    "                \"Otherwise you won't see the result! It's very important.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "python_tool.name = \"python_interpreter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "tools.append(search_tool)\n",
    "tools.append(python_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\py3921\\lib\\site-packages\\langsmith\\client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2288: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3 (Request ID: Root=1-68091d8d-53d6545f4f968e203edc9319;de43acb8-4c72-4741-8dee-5b1aa817f443)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mWe export oranges from Brazil.\u001b[39;49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;43m                                    In 2021 we exported 200t, in 2022 ‚Äì 190t, in 2023 ‚Äì 210t,\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;43m                                    in 2024, which has not yet ended ‚Äì 220t.\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;43m                                    Estimate how many oranges we can export in 2025,\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;43m                                    taking into account weather conditions in Brazil\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;43m                                    and the demand for oranges in the world, based on the economic situation.\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;43m                                    Make prediction and provide the result of the business analysis if you can do it.\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;43m                                    If not, write that you can\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mt do it\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain\\chains\\base.py:167\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    166\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain\\chains\\base.py:157\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 157\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    162\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain\\agents\\agent.py:1620\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m-> 1620\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m   1628\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[0;32m   1629\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[0;32m   1630\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain\\agents\\agent.py:1326\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1319\u001b[0m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1323\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1326\u001b[0m         [\n\u001b[0;32m   1327\u001b[0m             a\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1329\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1330\u001b[0m                 color_mapping,\n\u001b[0;32m   1331\u001b[0m                 inputs,\n\u001b[0;32m   1332\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1333\u001b[0m                 run_manager,\n\u001b[0;32m   1334\u001b[0m             )\n\u001b[0;32m   1335\u001b[0m         ]\n\u001b[0;32m   1336\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain\\agents\\agent.py:1326\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1319\u001b[0m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1323\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[1;32m-> 1326\u001b[0m         [\n\u001b[0;32m   1327\u001b[0m             a\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[0;32m   1329\u001b[0m                 name_to_tool_map,\n\u001b[0;32m   1330\u001b[0m                 color_mapping,\n\u001b[0;32m   1331\u001b[0m                 inputs,\n\u001b[0;32m   1332\u001b[0m                 intermediate_steps,\n\u001b[0;32m   1333\u001b[0m                 run_manager,\n\u001b[0;32m   1334\u001b[0m             )\n\u001b[0;32m   1335\u001b[0m         ]\n\u001b[0;32m   1336\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain\\agents\\agent.py:1354\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m   1351\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[0;32m   1353\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m-> 1354\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_agent\u001b[38;5;241m.\u001b[39mplan(\n\u001b[0;32m   1355\u001b[0m         intermediate_steps,\n\u001b[0;32m   1356\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1357\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m   1358\u001b[0m     )\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain\\agents\\agent.py:461\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[1;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_runnable:\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;66;03m# streaming\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    463\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_core\\runnables\\base.py:3424\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3417\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   3418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3419\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3422\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3423\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3424\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_core\\runnables\\base.py:3410\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3403\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   3404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3405\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3408\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3409\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3410\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3411\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3412\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3413\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3414\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3415\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_core\\runnables\\base.py:2205\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2205\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2206\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2207\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_core\\runnables\\base.py:3372\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3370\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3372\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_core\\runnables\\base.py:1419\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1416\u001b[0m final: Input\n\u001b[0;32m   1417\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ichunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m   1420\u001b[0m     \u001b[38;5;66;03m# The default implementation of transform is to buffer input and\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m     \u001b[38;5;66;03m# then call stream.\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;66;03m# It'll attempt to gather all input into a single chunk using\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;66;03m# the `+` operator.\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;66;03m# If the input is not addable, then we'll assume that we can\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;66;03m# only operate on the last chunk,\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;66;03m# and we'll iterate until we get to the last chunk.\u001b[39;00m\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[0;32m   1428\u001b[0m         final \u001b[38;5;241m=\u001b[39m ichunk\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_core\\runnables\\base.py:5632\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5625\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m   5626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   5627\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   5631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 5632\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   5633\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5635\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5636\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_core\\runnables\\base.py:1437\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1434\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1437\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_core\\language_models\\llms.py:559\u001b[0m, in \u001b[0;36mBaseLLM.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m generation: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    560\u001b[0m         prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    561\u001b[0m     ):\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:366\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._stream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream\u001b[39m(\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    359\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    363\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[GenerationChunk]:\n\u001b[0;32m    364\u001b[0m     invocation_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params(stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 366\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mtext_generation(\n\u001b[0;32m    367\u001b[0m         prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvocation_params, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    368\u001b[0m     ):\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;66;03m# identify stop sequence in generated text, if any\u001b[39;00m\n\u001b[0;32m    370\u001b[0m         stop_seq_found: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2392\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2367\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[0;32m   2368\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2369\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   2370\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2390\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[0;32m   2391\u001b[0m         )\n\u001b[1;32m-> 2392\u001b[0m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2394\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[0;32m   2395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\inference\\_common.py:410\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[1;34m(http_error)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2362\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2360\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   2361\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2362\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2363\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2364\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:357\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 357\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3921\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3 (Request ID: Root=1-68091d8d-53d6545f4f968e203edc9319;de43acb8-4c72-4741-8dee-5b1aa817f443)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "agent_executor.invoke({'input': \"\"\"We export oranges from Brazil.\n",
    "                                    In 2021 we exported 200t, in 2022 ‚Äì 190t, in 2023 ‚Äì 210t,\n",
    "                                    in 2024, which has not yet ended ‚Äì 220t.\n",
    "                                    Estimate how many oranges we can export in 2025,\n",
    "                                    taking into account weather conditions in Brazil\n",
    "                                    and the demand for oranges in the world, based on the economic situation.\n",
    "                                    Make prediction and provide the result of the business analysis if you can do it.\n",
    "                                    If not, write that you can't do it\"\"\"\n",
    "                                })"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
