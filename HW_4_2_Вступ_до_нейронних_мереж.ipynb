{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbLHTNfSclli"
   },
   "source": [
    "**Секція 1. Логістична регресія з нуля.**\n",
    "\n",
    "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
    "\n",
    "Давайте нагадаємо основні формули для логістичної регресії.\n",
    "\n",
    "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
    "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
    "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
    "- $ b $ — це зміщення (bias).\n",
    "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
    "\n",
    "### Як обчислюється сигмоїдна функція:\n",
    "\n",
    "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
    "\n",
    "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
    "\n",
    "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Де:\n",
    "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
    "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtOYB-RHfc_r"
   },
   "source": [
    "1.\n",
    "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3BNXSR-VdYKQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QLKZ77x4v_-v"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KjoeaDrk6fO7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "targets:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# Перетворення inputs, targets на torch тензори\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(\"inputs:\", inputs, sep = '\\n')\n",
    "print(\"targets:\", targets, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKzbJKfOgGV8"
   },
   "source": [
    "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aXhKw6Tdj1-d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1844b68ae90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eApcB7eb6h9o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
      "bias: tensor([0.6213], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Ініціація ваг w і зсуву b\n",
    "\n",
    "w = torch.randn(1, 3, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "print(\"weights:\", w)\n",
    "print(\"bias:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYGxNGTaf5s6"
   },
   "source": [
    "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
    "\n",
    "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
    "  - обчсилення $e^x$: `torch.exp(x)`\n",
    "  - обчсилення $log(x)$: `torch.log(x)`\n",
    "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
    "\n",
    "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
    "\n",
    "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pSz2j4Fh6jBv"
   },
   "outputs": [],
   "source": [
    "# Визначення функції гіпотези в логістичній регресії\n",
    "\n",
    "def model(x, w, b):\n",
    "    y = 1 / (1 + 1 / torch.exp(x @ w.t() + b))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Обчислення передбачень з поточними значеннями w, b\n",
    "\n",
    "preds = model(inputs, w, b)\n",
    "print(\"predictions:\", preds, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель повернула всі передбачення 1. Причина – невідповідні значення ваг і зсуву.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2AGM0Mb2yHa"
   },
   "source": [
    "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
    "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1bWlovvx6kZS"
   },
   "outputs": [],
   "source": [
    "# Визначення функції втрат\n",
    "\n",
    "def binary_cross_entropy(predicted_probs, true_labels):\n",
    "    loss = torch.mean(-(true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(nan, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Обчислення втрат для поточних передбачень моделі\n",
    "\n",
    "loss = binary_cross_entropy(preds, targets)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отримали невизначене значення функції втрат, оскільки маємо log(0), що дорівнює мінус нескінченність. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFKpQxdHi1__"
   },
   "source": [
    "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YAbXUNSJ6mCl"
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
      "w.grad: tensor([[nan, nan, nan]])\n"
     ]
    }
   ],
   "source": [
    "print(\"w:\", w)\n",
    "print(\"w.grad:\", w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: tensor([0.6213], requires_grad=True)\n",
      "b.grad: tensor([nan])\n"
     ]
    }
   ],
   "source": [
    "print(\"b:\", b)\n",
    "print(\"b.grad:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градієнти не визначені, ймовірно, дуже-дуже близькі до 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDN1t1RujQsK"
   },
   "source": [
    "**Що сталось?**\n",
    "\n",
    "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
    "\n",
    "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
    "\n",
    "   $$\n",
    "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "   $$\n",
    "\n",
    "\n",
    "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
    "\n",
    "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
    "\n",
    "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
    "\n",
    "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
    "\n",
    "**Що ж робити?**\n",
    "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
    "\n",
    "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
    "\n",
    "А я пишу пояснення, чому просто не зробити\n",
    "\n",
    "```\n",
    "w = torch.randn(1, 3, requires_grad=True)/1000\n",
    "b = torch.randn(1, requires_grad=True)/1000\n",
    "```\n",
    "\n",
    "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
    "\n",
    "1. **Що таке листовий тензор**\n",
    "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
    "\n",
    "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
    "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
    "\n",
    "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
    "\n",
    "3. **Чому важливо залишити тензор листовим**\n",
    "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
    "\n",
    "**Висновок:**\n",
    "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOPSQyttpVjO"
   },
   "source": [
    "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-EBOJ3tsnRaD"
   },
   "outputs": [],
   "source": [
    "torch.random.manual_seed(1)\n",
    "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
    "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
    "\n",
    "# in-place операції\n",
    "w.data = w.data / 1000\n",
    "b.data = b.data / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(inputs, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = binary_cross_entropy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
      "b: tensor([0.0006], requires_grad=True)\n",
      "predictions:\n",
      "tensor([[0.5174],\n",
      "        [0.5220],\n",
      "        [0.5244],\n",
      "        [0.5204],\n",
      "        [0.5190]], grad_fn=<MulBackward0>)\n",
      "loss: tensor(0.6829, grad_fn=<MeanBackward0>)\n",
      "w.grad: tensor([[ -5.4417, -18.9853, -10.0682]])\n",
      "b.grad: tensor([-0.0794])\n"
     ]
    }
   ],
   "source": [
    "print(\"w:\", w)\n",
    "print(\"b:\", b)\n",
    "print(\"predictions:\", preds, sep = '\\n')\n",
    "print(\"loss:\", loss)\n",
    "print(\"w.grad:\", w.grad)\n",
    "print(\"b.grad:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отримали скінченні значення функції втрат і градієнтів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCdi44IT334o"
   },
   "source": [
    "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
    "\n",
    "    1. Генерація прогнозів\n",
    "    2. Обчислення втрат\n",
    "    3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
    "    4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
    "    5. Скидання градієнтів на нуль\n",
    "\n",
    "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mObHPyE06qsO"
   },
   "outputs": [],
   "source": [
    "# Функція-алгоритм градієнтного спуску (модель тренується протягом 1000 епох)\n",
    "\n",
    "for i in range(1000):\n",
    "    preds = model(inputs, w, b)\n",
    "    loss = binary_cross_entropy(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets:\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "final predictions:\n",
      "tensor([[0.5777],\n",
      "        [0.6685],\n",
      "        [0.9113],\n",
      "        [0.1618],\n",
      "        [0.8653]], grad_fn=<MulBackward0>)\n",
      "final loss:\n",
      "tensor(0.3358, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"targets:\", targets, sep = '\\n')\n",
    "print(\"final predictions:\", preds, sep = '\\n')\n",
    "print(\"final loss:\", loss, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель помилилась лише в одному випадку – першому.\\\n",
    "Значення функції втрат покращилось – стало меншим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuRhlyF9qAia"
   },
   "source": [
    "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
    "\n",
    "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
    "\n",
    "Даних у нас буде побільше - тож, визначаємо нові масиви."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IX8Bhm74rV4M"
   },
   "outputs": [],
   "source": [
    "# Вхідні дані (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70],\n",
    "                   [73, 67, 43],\n",
    "                   [91, 88, 64],\n",
    "                   [87, 134, 58],\n",
    "                   [102, 43, 37],\n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Таргети (apples > 80)\n",
    "targets = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0],\n",
    "                    [1]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X2dV30KtAPu"
   },
   "source": [
    "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "chrvMfBs6vjo"
   },
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Визначаємо dataset\n",
    "\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nMFaa8suOd3"
   },
   "source": [
    "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZCsRo5Mx6wEI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [102.,  43.,  37.],\n",
       "         [ 69.,  96.,  70.],\n",
       "         [102.,  43.,  37.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Визначаємо data loader\n",
    "\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle = True)\n",
    "next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymcQOo_hum6I"
   },
   "source": [
    "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
    "\n",
    "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
    "\n",
    "  Створіть екземпляр класу `LogReg` в змінній `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EyAwhTBW6xxz"
   },
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogReg(\n",
       "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogReg()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RflV7xeVyoJy"
   },
   "source": [
    "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3QCATPU_6yfa"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "loss_fn = F.binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.6312, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(inputs), targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch-WrYnKzMzq"
   },
   "source": [
    "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cEHQH9qE626k"
   },
   "outputs": [],
   "source": [
    "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Ініціалізуємо акумулятор для втрат\n",
    "        total_loss = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            # Генеруємо передбачення\n",
    "            pred = model(xb)\n",
    "\n",
    "            # Обчислюємо втрати\n",
    "            loss = loss_fn(pred, yb)\n",
    "\n",
    "            # Виконуємо градієнтний спуск\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # Накопичуємо втрати\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Обчислюємо середні втрати для епохи\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        # Виводимо підсумок епохи\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 6.6400\n",
      "Epoch [20/1000], Loss: 6.0483\n",
      "Epoch [30/1000], Loss: 5.7263\n",
      "Epoch [40/1000], Loss: 5.4710\n",
      "Epoch [50/1000], Loss: 5.2474\n",
      "Epoch [60/1000], Loss: 5.0653\n",
      "Epoch [70/1000], Loss: 4.8974\n",
      "Epoch [80/1000], Loss: 4.7861\n",
      "Epoch [90/1000], Loss: 4.6903\n",
      "Epoch [100/1000], Loss: 4.5353\n",
      "Epoch [110/1000], Loss: 4.4753\n",
      "Epoch [120/1000], Loss: 4.3686\n",
      "Epoch [130/1000], Loss: 4.2792\n",
      "Epoch [140/1000], Loss: 4.2018\n",
      "Epoch [150/1000], Loss: 4.1132\n",
      "Epoch [160/1000], Loss: 4.0389\n",
      "Epoch [170/1000], Loss: 3.9423\n",
      "Epoch [180/1000], Loss: 3.8701\n",
      "Epoch [190/1000], Loss: 3.7756\n",
      "Epoch [200/1000], Loss: 3.6974\n",
      "Epoch [210/1000], Loss: 3.6101\n",
      "Epoch [220/1000], Loss: 3.5297\n",
      "Epoch [230/1000], Loss: 3.4427\n",
      "Epoch [240/1000], Loss: 3.3760\n",
      "Epoch [250/1000], Loss: 3.2822\n",
      "Epoch [260/1000], Loss: 3.1985\n",
      "Epoch [270/1000], Loss: 3.1111\n",
      "Epoch [280/1000], Loss: 3.0308\n",
      "Epoch [290/1000], Loss: 2.9483\n",
      "Epoch [300/1000], Loss: 2.8748\n",
      "Epoch [310/1000], Loss: 2.7879\n",
      "Epoch [320/1000], Loss: 2.7048\n",
      "Epoch [330/1000], Loss: 2.6292\n",
      "Epoch [340/1000], Loss: 2.5456\n",
      "Epoch [350/1000], Loss: 2.4710\n",
      "Epoch [360/1000], Loss: 2.4122\n",
      "Epoch [370/1000], Loss: 2.3102\n",
      "Epoch [380/1000], Loss: 2.2363\n",
      "Epoch [390/1000], Loss: 2.1587\n",
      "Epoch [400/1000], Loss: 2.0854\n",
      "Epoch [410/1000], Loss: 2.0062\n",
      "Epoch [420/1000], Loss: 1.9342\n",
      "Epoch [430/1000], Loss: 1.8632\n",
      "Epoch [440/1000], Loss: 1.7922\n",
      "Epoch [450/1000], Loss: 1.7232\n",
      "Epoch [460/1000], Loss: 1.6485\n",
      "Epoch [470/1000], Loss: 1.5853\n",
      "Epoch [480/1000], Loss: 1.5150\n",
      "Epoch [490/1000], Loss: 1.4520\n",
      "Epoch [500/1000], Loss: 1.3831\n",
      "Epoch [510/1000], Loss: 1.3171\n",
      "Epoch [520/1000], Loss: 1.2595\n",
      "Epoch [530/1000], Loss: 1.2084\n",
      "Epoch [540/1000], Loss: 1.1388\n",
      "Epoch [550/1000], Loss: 1.0851\n",
      "Epoch [560/1000], Loss: 1.0415\n",
      "Epoch [570/1000], Loss: 0.9843\n",
      "Epoch [580/1000], Loss: 0.9393\n",
      "Epoch [590/1000], Loss: 0.8865\n",
      "Epoch [600/1000], Loss: 0.8436\n",
      "Epoch [610/1000], Loss: 0.8030\n",
      "Epoch [620/1000], Loss: 0.7632\n",
      "Epoch [630/1000], Loss: 0.7308\n",
      "Epoch [640/1000], Loss: 0.7033\n",
      "Epoch [650/1000], Loss: 0.6679\n",
      "Epoch [660/1000], Loss: 0.6437\n",
      "Epoch [670/1000], Loss: 0.6134\n",
      "Epoch [680/1000], Loss: 0.5918\n",
      "Epoch [690/1000], Loss: 0.5703\n",
      "Epoch [700/1000], Loss: 0.5492\n",
      "Epoch [710/1000], Loss: 0.5325\n",
      "Epoch [720/1000], Loss: 0.5149\n",
      "Epoch [730/1000], Loss: 0.5012\n",
      "Epoch [740/1000], Loss: 0.4870\n",
      "Epoch [750/1000], Loss: 0.4743\n",
      "Epoch [760/1000], Loss: 0.4654\n",
      "Epoch [770/1000], Loss: 0.4552\n",
      "Epoch [780/1000], Loss: 0.4421\n",
      "Epoch [790/1000], Loss: 0.4315\n",
      "Epoch [800/1000], Loss: 0.4263\n",
      "Epoch [810/1000], Loss: 0.4149\n",
      "Epoch [820/1000], Loss: 0.4085\n",
      "Epoch [830/1000], Loss: 0.4013\n",
      "Epoch [840/1000], Loss: 0.3940\n",
      "Epoch [850/1000], Loss: 0.3883\n",
      "Epoch [860/1000], Loss: 0.3829\n",
      "Epoch [870/1000], Loss: 0.3770\n",
      "Epoch [880/1000], Loss: 0.3719\n",
      "Epoch [890/1000], Loss: 0.3672\n",
      "Epoch [900/1000], Loss: 0.3627\n",
      "Epoch [910/1000], Loss: 0.3587\n",
      "Epoch [920/1000], Loss: 0.3549\n",
      "Epoch [930/1000], Loss: 0.3522\n",
      "Epoch [940/1000], Loss: 0.3474\n",
      "Epoch [950/1000], Loss: 0.3445\n",
      "Epoch [960/1000], Loss: 0.3411\n",
      "Epoch [970/1000], Loss: 0.3395\n",
      "Epoch [980/1000], Loss: 0.3350\n",
      "Epoch [990/1000], Loss: 0.3319\n",
      "Epoch [1000/1000], Loss: 0.3297\n"
     ]
    }
   ],
   "source": [
    "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]], grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs).round() == targets"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
